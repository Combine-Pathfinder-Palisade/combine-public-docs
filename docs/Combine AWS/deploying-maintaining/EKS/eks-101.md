# EKS 101

What is EKS? How do you support it? It's a lot of fun!

Raw content from discussions with Matt:



Notes from EKS Session with Matt:
- If a new customer wants/needs to use combine imds proxy (we only want to give it to them if the NEED to use IMDS, but it’ll require a code change anyway, so they might not want to use it after all)
  - pull it down from our public ECR, push back up to their own ECR from within Combine
  - Action Item for each of us - look at the combine-proxy-imds repo, the docs are very nice
- learn how the customer is deploying EKS
  - through terraform, or some other means
  - there’s configuration in terraform vs just EKS in the console
    - more often than not customers use terraform
- Things to tell them
  - EKS nodes need to be able to trust the Combine CA chain (like we do for normal VMs)
  - A lot of things around IRSA (IAM Roles for Service Accounts)
    - `IRSA` needs to have the emulated IAM ARNs, because they run on the nodes inside the Combine VPC
    - `aws-auth` config map needs to be commercial
  - only thing in the kubeconfig that gets emulated is the server endpoint url
  - EBS-CSI driver versions newer than 1.32 return a new CreateVolume topology name
    - but we got around this by adding the user agent fix (Daniel will update this message with a link to that fix, it was with SF before Stephen left)
  - Miscellaneous
    - there are cases where customers will need an ingress subnet
      - there’s a load balancer controller plugin that can set up load balancers as ingresses into the cluster
  - IMDS Proxy improvements (IMDS PROXY DEPRECATED, DO NOT TALK ABOUT IT)
    - some endpoints needs to be updated
  - When a pod stands up in k8s, it doesn’t have an IP. So when it makes its first call to the IMDS proxy, it’s IP might not be in the list of approved IPs; the proxy might reject it. As part of that process, proxy will refresh its list of IPs, then if it’s still not in there, then it’ll reject it. The refresh might be fast enough that it ‘beats’ the IP assigned to the pod
    - in the case of the ebs-csi driver, it only tries twice before giving up
    - i.e. there needs to be an improvement with how the refresh works, maybe set up an exponential backoff

Some good resources
- IRSA - IAM roles for Service Accounts Article - https://medium.com/@ankit.wal/the-how-of-iam-roles-for-service-accounts-irsa-on-aws-eks-3d76badb8942
- 4 hour YouTube course on Vanilla Kubernetes - https://www.youtube.com/watch?v=X48VuDVv0do
- Full EKS course on Kubernetes (don’t know of any right now)
- the kubernetes config parameters in the emulation.config in combine-aws
- the kubernetes rewriters in combine-aws
- `combine-imds-proxy` repository (only for historical purposes)


------
the cluster role in cisco has these permissions
- eks cluster policy
- eks cluster vpc controller
- allow for KMS, required for EKS encryption
  - there's no way to set EKS encryption at the customer for TS
  - the permissions on WLDEVELOPER do not support creating a KMS key grant, and the permissions on the KeyManager role do not let them list eks clusters
- deny for resource groups

Users can create roles as long as they're prefixed by PROJECT_ and bounded by the Permissions Boundary `PB-WLDEVELOPER`, required by S or TS customer

EVERY ROLE related to EKS has to be prefixed by PROJECT_

TO stand up a cluster
1. create role
2. create cluster
3. create oidc provider
  - not done by WLDEVELOPER, (Cisco uses CUSTOMERIT) or a Sequoia admin

Repeat steps 1 & 2 for the node groups,
1. Create node group role(s)
2. create node groups

For VPC-only clusters: if the SG on the cluster does not include the cidr block of the node groups, the node groups will not be able to join the cluster. Does not apply to public clusters.

OIDC provider is there for IRSA. service accounts can use the OIDC provider to assume a role in AWS

once the cluster is created, you take the OIDC provider url and put in the trust policy

trust policy for a PROJECT_aws-ebs-csi-driver-controller POD role:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::663117128738:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/7435A97694DAE25C2079AC8D8B66D242" oidc provider arn
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.us-east-1.amazonaws.com/id/7435A97694DAE25C2079AC8D8B66D242:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa", needs to be the 'path' of the service account 
                    "oidc.eks.us-east-1.amazonaws.com/id/7435A97694DAE25C2079AC8D8B66D242:aud": "sts.amazonaws.com" needs to match the sts endpoint on the oidc provider, 
                }
            }
        },
        {
            "Sid": "CombineInjectedServicePrincipal",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::663117128738:root"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

Service Account sample:
```
{
  "kind": "ServiceAccount",
  "apiVersion": "v1",
  "metadata": {
    "name": "ebs-csi-controller-sa",
    "namespace": "kube-system",
    "uid": "8339dcbd-119b-4f5f-84ae-a1e7a8c24192",
    "resourceVersion": "179402",
    "creationTimestamp": "2024-09-18T21:56:08Z",
    "labels": {
      "app.kubernetes.io/component": "csi-driver",
      "app.kubernetes.io/instance": "aws-ebs-csi-driver",
      "app.kubernetes.io/managed-by": "Helm",
      "app.kubernetes.io/name": "aws-ebs-csi-driver",
      "app.kubernetes.io/version": "1.33.0",
      "helm.sh/chart": "aws-ebs-csi-driver-2.33.0"
    },
    "annotations": {
      "eks.amazonaws.com/audience": "sts.amazonaws.com",
      "eks.amazonaws.com/role-arn": "arn:aws-iso:iam::663117128738:role/PROJECT_aws-ebs-csi-driver-controller",
      "meta.helm.sh/release-name": "aws-ebs-csi-driver",
      "meta.helm.sh/release-namespace": "kube-system"
    }
  },
  "automountServiceAccountToken": true
}
```

the annotations matter, the audience has to match the sts endpoint from above, the role arn needs to match the role
the pod will get created with an sts token that has the role

here's a pod that's using the irsa
 the serviceAccountToken listed in the voume needs to match the audience for the STS account token
 the aws-iam-token volume is the one to look at
 this automount is a flag in most helm charts https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/charts/aws-ebs-csi-driver/values.yaml#L397

The terraform in `combine-terraform` does this; release sparingly to customers, as it is very complex and requires helm and other stuff.
